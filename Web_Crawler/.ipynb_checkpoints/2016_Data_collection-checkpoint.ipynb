{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 蘋果日報 already sampled\n",
    "# 蔡英文 自由時報 2016\n",
    "\n",
    "tsai_lib_url = ['https://news.ltn.com.tw/search?keyword=%E8%94%A1%E8%8B%B1%E6%96%87&conditions=and&start_time=2015-11-23&end_time=2016-01-14&page={}'.format(i) for i in range(1, 209)]\n",
    "headers = []\n",
    "news_url = []\n",
    "time_dt = []\n",
    "media = []\n",
    "\n",
    "for i in tsai_lib_url:\n",
    "        res_news = requests.get(i)\n",
    "        newslist_soup = BeautifulSoup(res_news.text, 'lxml')\n",
    "\n",
    "        for newslist in newslist_soup.find_all('div', class_ = 'whitecon'):\n",
    "            li_label = newslist.find_all('li')\n",
    "\n",
    "          # 新聞標題\n",
    "        for t in li_label:\n",
    "            headers.append(t.find('a').text)\n",
    "\n",
    "        # 新聞連結\n",
    "        url_sect = [t.find('a') for t in li_label]\n",
    "        for t in url_sect:\n",
    "            news_url.append(t.get('href'))\n",
    "\n",
    "        # 時間\n",
    "        for t in li_label:\n",
    "            time_dt.append(t.find('span').text)\n",
    "\n",
    "        time.sleep(2)\n",
    "    \n",
    "media = []\n",
    "for i in range(len(headers)):\n",
    "    media.append('Liberal Times')\n",
    "\n",
    "time_nw_tsai = []\n",
    "for i in time_dt:\n",
    "    time_nw_tsai.append(i.replace(i[10:], ''))\n",
    "time_nw_tsai\n",
    "\n",
    "df_tsai_lib = pd.DataFrame(\n",
    "{\n",
    "    'titles' : headers,\n",
    "    'links' : news_url,\n",
    "    'time' : time_nw_tsai,\n",
    "    'media': media\n",
    "})\n",
    "df_tsai_lib.drop_duplicates('titles', 'first', inplace = True)\n",
    "\n",
    "df_tsai_lib.to_csv('/Users/garday/Documents/MY499/Data Collection/Data/2016/蔡英文_2016_lib.csv', \\\n",
    "                  encoding = 'utf_8_sig', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朱立倫 自由時報\n",
    "headers_c = []\n",
    "news_url_c = []\n",
    "time_dt_c = []\n",
    "media_c = []\n",
    "chu_lib_url = ['https://news.ltn.com.tw/search?keyword=%E6%9C%B1%E7%AB%8B%E5%80%AB&conditions=and&start_time=2015-11-23&end_time=2016-01-14&page='.format(i) for i in range(1, 179)]\n",
    "for i in chu_lib_url:\n",
    "        res_news = requests.get(i)\n",
    "        newslist_soup = BeautifulSoup(res_news.text, 'lxml')\n",
    "\n",
    "        for newslist in newslist_soup.find_all('div', class_ = 'whitecon'):\n",
    "            li_label = newslist.find_all('li')\n",
    "\n",
    "          # 新聞標題\n",
    "        for t in li_label:\n",
    "            headers_c.append(t.find('a').text)\n",
    "\n",
    "        # 新聞連結\n",
    "        url_sect = [t.find('a') for t in li_label]\n",
    "        for t in url_sect:\n",
    "            news_url_c.append(t.get('href'))\n",
    "\n",
    "        # 時間\n",
    "        for t in li_label:\n",
    "            time_dt_c.append(t.find('span').text)\n",
    "\n",
    "        time.sleep(2)\n",
    "    \n",
    "for i in range(len(headers_c)):\n",
    "    media_c.append('Liberal Times')\n",
    "\n",
    "len(headers_c)\n",
    "\n",
    "time_nw_chu = []\n",
    "for i in time_dt_c:\n",
    "    time_nw_chu.append(i.replace(i[10:], ''))\n",
    "\n",
    "df_chu_lib = pd.DataFrame(\n",
    "{\n",
    "    'titles' : headers_c,\n",
    "    'links' : news_url_c,\n",
    "    'time' : time_nw_chu,\n",
    "    'media': media_c\n",
    "})\n",
    "df_chu_lib.drop_duplicates('titles', 'first', inplace = True)\n",
    "\n",
    "df_chu_lib.to_csv('/Users/garday/Documents/MY499/Data Collection/Data/2016/朱立倫_2016_lib.csv', \\\n",
    "                  encoding = 'utf_8_sig', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 蔡英文 聯合報\n",
    "tsai_url_udn = ['https://udndata.com/ndapp/Searchdec?udndbid=udnfree&page={}&SearchString=%BD%B2%AD%5E%A4%E5%2B%A4%E9%B4%C1%3E%3D20151123%2B%A4%E9%B4%C1%3C%3D20160114%2B%B3%F8%A7%4F%3D%C1%70%A6%58%B3%F8%7C%B8%67%C0%D9%A4%E9%B3%F8%7C%C1%70%A6%58%B1%DF%B3%F8%7CUpaper&sharepage=50&select=1&kind=2&showSearchString='.format(i) for i in range(1, 24)]\n",
    "\n",
    "headers_tsai_udn = []\n",
    "news_url_tsai_udn = []\n",
    "time_dt_tsai_udn = []\n",
    "time_nw_tsai_udn = []\n",
    "\n",
    "for i in tsai_url_udn:\n",
    "    udn_res = requests.get(i)\n",
    "    udn_soup = BeautifulSoup(udn_res.text, 'lxml')\n",
    "    udn_newslist= udn_soup.find_all('div', class_ = 'list')\n",
    "    for t in udn_newslist:\n",
    "        title_sect = t.find_all('h2')\n",
    "\n",
    "    for t in title_sect:\n",
    "        headers_tsai_udn.append(t.find('a').text)\n",
    "\n",
    "    url_sect = [t.find('a') for t in title_sect]\n",
    "    for url in url_sect:\n",
    "        news_url_tsai_udn.append('https://udn.com' + url['href'])\n",
    "\n",
    "    for t in udn_newslist:\n",
    "        time_tag = t.find_all('span')\n",
    "    for t in time_tag:\n",
    "        time_dt_tsai_udn.append(t.text)\n",
    "    for i in time_dt_tsai_udn:\n",
    "        time_nw_tsai_udn.append(i.replace(i[10:], ''))\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "media_tsai_udn= []\n",
    "for i in range(len(headers_tsai_udn)):\n",
    "    media_tsai_udn.append('UDN')\n",
    "    \n",
    "time_cl_tsai = []\n",
    "for i in time_dt_tsai_udn:\n",
    "    time_cl_tsai.append(i.replace(i[10:], ''))\n",
    "    \n",
    "df_tyw_udn = pd.DataFrame(\n",
    "{\n",
    "    'titles' : headers_tsai_udn,\n",
    "    'links' : news_url_tsai_udn,\n",
    "    'time' : time_cl_tsai,\n",
    "    'media': media_tsai_udn\n",
    "})\n",
    "df_tyw_udn.drop_duplicates('titles', 'first', inplace = True)\n",
    "df_tyw_udn.to_csv('/Users/garday/Documents/MY499/Data Collection/Data/2016/蔡英文_2016_udn.csv', \\\n",
    "                  encoding = 'utf_8_sig', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朱立倫 聯合報\n",
    "chu_url_udn = ['https://udndata.com/ndapp/Searchdec?udndbid=udnfree&page={}&SearchString=%A6%B6%A5%DF%AD%DB%2B%A4%E9%B4%C1%3E%3D20151123%2B%A4%E9%B4%C1%3C%3D20160114%2B%B3%F8%A7%4F%3D%C1%70%A6%58%B3%F8%7C%B8%67%C0%D9%A4%E9%B3%F8%7C%C1%70%A6%58%B1%DF%B3%F8%7CUpaper&sharepage=50&select=1&kind=2'.format(i) for i in range(1, 17)]\n",
    "\n",
    "headers_chu_udn = []\n",
    "news_url_chu_udn = []\n",
    "time_dt_chu_udn = []\n",
    "time_nw_chu_udn = []\n",
    "\n",
    "for i in chu_url_udn:\n",
    "    udn_res = requests.get(i)\n",
    "    udn_soup = BeautifulSoup(udn_res.text, 'lxml')\n",
    "    udn_newslist= udn_soup.find_all('div', class_ = 'list')\n",
    "    for t in udn_newslist:\n",
    "        title_sect = t.find_all('h2')\n",
    "\n",
    "    for t in title_sect:\n",
    "        headers_chu_udn.append(t.find('a').text)\n",
    "\n",
    "    url_sect = [t.find('a') for t in title_sect]\n",
    "    for url in url_sect:\n",
    "        news_url_chu_udn.append('https://udn.com' + url['href'])\n",
    "\n",
    "    for t in udn_newslist:\n",
    "        time_tag = t.find_all('span')\n",
    "    for t in time_tag:\n",
    "        time_dt_chu_udn.append(t.text)\n",
    "    for i in time_dt_chu_udn:\n",
    "        time_nw_chu_udn.append(i.replace(i[10:], ''))\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "media_chu_udn= []\n",
    "for i in range(len(headers_chu_udn)):\n",
    "    media_chu_udn.append('UDN')\n",
    "    \n",
    "time_cl_chu = []\n",
    "for i in time_dt_chu_udn:\n",
    "    time_cl_chu.append(i.replace(i[10:], ''))\n",
    "    \n",
    "df_chu_udn = pd.DataFrame(\n",
    "{\n",
    "    'titles' : headers_chu_udn,\n",
    "    'links' : news_url_chu_udn,\n",
    "    'time' : time_cl_chu,\n",
    "    'media': media_chu_udn\n",
    "})\n",
    "df_chu_udn.drop_duplicates('titles', 'first', inplace = True)\n",
    "df_tyw_udn.to_csv('/Users/garday/Documents/MY499/Data Collection/Data/2016/朱立倫_2016_udn.csv', \\\n",
    "                  encoding = 'utf_8_sig', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a3c81ca94891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m'links'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnews_url_cht_tsai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m'time'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtime_dt_cht_tsai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;34m'media'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmedia_cht_tsai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m })\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arrays must all be same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "# 蔡英文 中國時報\n",
    "tsai_cht = ['https://www.chinatimes.com/search/%E8%94%A1%E8%8B%B1%E6%96%87?page={}&chdtv'.format(i) for i in range(1417, 1465)]\n",
    "\n",
    "headers_cht_tsai = []\n",
    "news_url_cht_tsai = []\n",
    "time_dt_cht_tsai = []\n",
    "\n",
    "for i in tsai_cht:\n",
    "    res_news = requests.get(i)\n",
    "    newslist_soup = BeautifulSoup(res_news.text, 'lxml')\n",
    "\n",
    "    for newslist in newslist_soup.find_all('div', class_ = 'item-list article-list'):\n",
    "        title_sect = newslist.find_all('h3')\n",
    "\n",
    "    for t in title_sect:\n",
    "        headers_cht_tsai.append(t.find('a').text)\n",
    "\n",
    "    url_sect = [t.find('a') for t in title_sect]\n",
    "    for t in url_sect:\n",
    "        news_url_cht_tsai.append(t.get('href'))\n",
    "\n",
    "    for i in newslist_soup.find_all('div', class_ = 'item-list article-list'):\n",
    "        time_test = i.find_all('div', class_ = 'meta-info')\n",
    "    for t in time_test:\n",
    "        time_dt_cht_tsai.append(t.find('time').text)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "media_cht_tsai = []\n",
    "for i in range(len(headers_cht_tsai)):\n",
    "    media_cht_tsai.append('China Times')\n",
    "    \n",
    "time_tsai_cht = []\n",
    "for i in time_dt_cht_tsai:\n",
    "    time_tsai_cht.append(i.replace(i[:5], ''))\n",
    "df_tsai_cht = pd.DataFrame(\n",
    "{\n",
    "    'titles' : headers_cht_tsai,\n",
    "    'links' : news_url_cht_tsai,\n",
    "    'time' : time_dt_cht_tsai,\n",
    "    'media': media_cht_tsai\n",
    "})\n",
    "\n",
    "df_tsai_cht.drop_duplicates('titles', 'first', inplace = True)\n",
    "df_tsai_cht.to_csv('/Users/garday/Documents/MY499/Data Collection/Data/2016/蔡英文_2016_cht.csv', \\\n",
    "                  encoding = 'utf_8_sig', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7a13eb1b78b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m'links'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnews_url_cht_chu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m'time'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtime_dt_cht_chu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;34m'media'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmedia_cht_chu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m })\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arrays must all be same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "# 朱立倫 中國時報 \n",
    "\n",
    "# 還沒跑完\n",
    "chu_url = ['https://www.chinatimes.com/search/%E6%9C%B1%E7%AB%8B%E5%80%AB?page={}&chdtv'.format(i) for i in range(185, 220)]\n",
    "headers_cht_chu = []\n",
    "news_url_cht_chu = []\n",
    "time_dt_cht_chu= []\n",
    "\n",
    "for i in chu_url:\n",
    "    res_news = requests.get(i)\n",
    "    newslist_soup = BeautifulSoup(res_news.text, 'lxml')\n",
    "\n",
    "    for newslist in newslist_soup.find_all('div', class_ = 'item-list article-list'):\n",
    "        title_sect = newslist.find_all('h3')\n",
    "\n",
    "    for t in title_sect:\n",
    "        headers_cht_chu.append(t.find('a').text)\n",
    "\n",
    "    url_sect = [t.find('a') for t in title_sect]\n",
    "    for t in url_sect:\n",
    "        news_url_cht_chu.append(t.get('href'))\n",
    "\n",
    "    for i in newslist_soup.find_all('div', class_ = 'item-list article-list'):\n",
    "        time_test = i.find_all('div', class_ = 'meta-info')\n",
    "    for t in time_test:\n",
    "        time_dt_cht_chu.append(t.find('time').text)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "media_cht_chu = []\n",
    "for i in range(len(headers_cht_chu)):\n",
    "    media_cht_tsai.append('China Times')\n",
    "    \n",
    "time_tsai_chu = []\n",
    "for i in time_dt_cht_chu:\n",
    "    time_tsai_chu.append(i.replace(i[:5], ''))\n",
    "df_chu_cht = pd.DataFrame(\n",
    "{\n",
    "    'titles' : headers_cht_chu,\n",
    "    'links' : news_url_cht_chu,\n",
    "    'time' : time_dt_cht_chu,\n",
    "    'media': media_cht_chu\n",
    "})\n",
    "\n",
    "df_chu_cht.drop_duplicates('titles', 'first', inplace = True)\n",
    "df_chu_cht.to_csv('/Users/garday/Documents/MY499/Data Collection/Data/2016/朱立倫_2016_cht.csv', \\\n",
    "                  encoding = 'utf_8_sig', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

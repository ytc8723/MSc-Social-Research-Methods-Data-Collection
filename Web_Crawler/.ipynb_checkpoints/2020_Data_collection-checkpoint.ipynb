{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pprint\n",
    "import re\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import scrapy\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 蘋果日報 sampled\n",
    "#自由時報 蔡英文2020\n",
    "def liberal_times_webcrawl(url):\n",
    "    \n",
    "    headers = []\n",
    "    news_url = []\n",
    "    #article = []\n",
    "    time_dt = []\n",
    "    for page_num in range(1,266):\n",
    "        res_news = requests.get(url + repr(page_num))\n",
    "        newslist_soup = BeautifulSoup(res_news.text, 'lxml')\n",
    "\n",
    "        for newslist in newslist_soup.find_all('div', class_ = 'whitecon'):\n",
    "            li_label = newslist.find_all('li')\n",
    "\n",
    "          # 新聞標題\n",
    "        for t in li_label:\n",
    "            headers.append(t.find('a').text)\n",
    "\n",
    "        # 新聞連結\n",
    "        url_sect = [t.find('a') for t in li_label]\n",
    "        for t in url_sect:\n",
    "            news_url.append(t.get('href'))\n",
    "\n",
    "        # 新聞abstract\n",
    "        p = [p.find('p') for p in li_label]\n",
    "\n",
    "        for content in p: \n",
    "            article.append(content.text)\n",
    "\n",
    "        # 時間\n",
    "        for t in li_label:\n",
    "            time_dt.append(t.find('span').text)\n",
    "\n",
    "        time.sleep(2)\n",
    "    \n",
    "    media = []\n",
    "    for i in range(len(time_dt)):\n",
    "        media.append('Liberal Times')\n",
    "        \n",
    "    return headers, news_url, article, time_dt, media \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_t_l_2020 = 'https://news.ltn.com.tw/search?keyword=%E8%94%A1%E8%8B%B1%E6%96%87&conditions=and&start_time=2019-11-23&end_time=2020-01-10&page='\n",
    "\n",
    "tsy_2020_lib = liberal_times_webcrawl(url_t_l_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中國時報\n",
    "def china_times_webcrawler(url):\n",
    "    headers = []\n",
    "    news_url = []\n",
    "    time_dt = []\n",
    "    #time_nw = []\n",
    "\n",
    "    for page_num in range(135, 342):\n",
    "        res_news = requests.get(url + repr(page_num) + '&chdtv')\n",
    "        newslist_soup = BeautifulSoup(res_news.text, 'lxml')\n",
    "\n",
    "        for newslist in newslist_soup.find_all('div', class_ = 'item-list article-list'):\n",
    "            title_sect = newslist.find_all('h3')\n",
    "\n",
    "        for t in title_sect:\n",
    "            headers.append(t.find('a').text)\n",
    "\n",
    "        url_sect = [t.find('a') for t in title_sect]\n",
    "        for t in url_sect:\n",
    "            news_url.append(t.get('href'))\n",
    "\n",
    "        for i in newslist_soup.find_all('div', class_ = 'item-list article-list'):\n",
    "            time_test = i.find_all('div', class_ = 'meta-info')\n",
    "        for t in time_test:\n",
    "            time_dt.append(t.find('time').text)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    media = []\n",
    "    for i in range(len(headers)):\n",
    "        media.append('China Times')\n",
    "\n",
    "    return headers, news_url, time_dt, media\n",
    "\n",
    "url = 'https://www.chinatimes.com/search/%E8%94%A1%E8%8B%B1%E6%96%87?page='\n",
    "# url2 = 'https://www.chinatimes.com/search/%E8%94%A1%E8%8B%B1%E6%96%87?page=342&chdtv'\n",
    "tyw_china_times = china_times_webcrawler(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cl = []\n",
    "for i in tyw_china_times[2]:\n",
    "    time_cl.append(i.replace(i[:5], ''))\n",
    "    \n",
    "df_tyw_cht = pd.DataFrame(\n",
    "{\n",
    "    'titles' : tyw_china_times[0],\n",
    "    'links' : tyw_china_times[1],\n",
    "    'time' : time_cl,\n",
    "    'media': tyw_china_times[3]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tyw_cht.drop_duplicates('titles', 'first', inplace = True)\n",
    "df_tyw_cht.to_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/蔡英文_2020_chinatimes.csv', \\\n",
    "                  encoding = 'utf_8_sig', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 韓國瑜 中國時報\n",
    "def china_times_webcrawler(url):\n",
    "    headers = []\n",
    "    news_url = []\n",
    "    time_dt = []\n",
    "\n",
    "    for page_num in range(184, 411):\n",
    "        res_news = requests.get(url + repr(page_num) + '&chdtv')\n",
    "        newslist_soup = BeautifulSoup(res_news.text, 'lxml')\n",
    "\n",
    "        for newslist in newslist_soup.find_all('div', class_ = 'item-list article-list'):\n",
    "            title_sect = newslist.find_all('h3')\n",
    "\n",
    "        for t in title_sect:\n",
    "            headers.append(t.find('a').text)\n",
    "\n",
    "        url_sect = [t.find('a') for t in title_sect]\n",
    "        for t in url_sect:\n",
    "            news_url.append(t.get('href'))\n",
    "\n",
    "        for i in newslist_soup.find_all('div', class_ = 'item-list article-list'):\n",
    "            time_test = i.find_all('div', class_ = 'meta-info')\n",
    "        for t in time_test:\n",
    "            time_dt.append(t.find('time').text)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    media = []\n",
    "    for i in range(len(headers)):\n",
    "        media.append('China Times')\n",
    "\n",
    "    return headers, news_url, time_dt, media\n",
    "url_h = 'https://www.chinatimes.com/search/%E9%9F%93%E5%9C%8B%E7%91%9C?page='\n",
    "\n",
    "hky_cht = china_times_webcrawler(url_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 韓國瑜 中國時報\n",
    "\n",
    "time_cl_h = []\n",
    "for i in hky_cht[2]:\n",
    "    time_cl_h.append(i.replace(i[:5], ''))\n",
    "df_hky_cht = pd.DataFrame(\n",
    "{\n",
    "    'titles' : hky_cht[0],\n",
    "    'links' : hky_cht[1],\n",
    "    'time' : time_cl_h,\n",
    "    'media': hky_cht[3]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 韓國瑜 中國時報\n",
    "\n",
    "df_hky_cht.drop_duplicates('titles', 'first', inplace = True)\n",
    "df_hky_cht.to_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/韓國瑜_2020_chinatimes.csv', \\\n",
    "                  encoding = 'utf_8_sig', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 蔡英文 udn 2020\n",
    "url_list = ['https://udndata.com/ndapp/Searchdec?udndbid=udnfree&page={}&SearchString=%BD%B2%AD%5E%A4%E5%2B%A4%E9%B4%C1%3E%3D20191123%2B%A4%E9%B4%C1%3C%3D20200110%2B%B3%F8%A7%4F%3D%C1%70%A6%58%B3%F8%7C%B8%67%C0%D9%A4%E9%B3%F8%7C%C1%70%A6%58%B1%DF%B3%F8%7CUpaper&sharepage=50&select=1&kind=2'.format(str(i)) for i in range(1,25)]\n",
    "\n",
    "headers = []\n",
    "news_url = []\n",
    "time_dt = []\n",
    "time_nw = []\n",
    "\n",
    "for i in url_list:\n",
    "    udn_res = requests.get(i)\n",
    "    udn_soup = BeautifulSoup(udn_res.text, 'lxml')\n",
    "    udn_newslist= udn_soup.find_all('div', class_ = 'list')\n",
    "    for t in udn_newslist:\n",
    "        title_sect = t.find_all('h2')\n",
    "\n",
    "    for t in title_sect:\n",
    "        headers.append(t.find('a').text)\n",
    "\n",
    "    url_sect = [t.find('a') for t in title_sect]\n",
    "    for url in url_sect:\n",
    "        news_url.append('https://udn.com' + url['href'])\n",
    "\n",
    "    for t in udn_newslist:\n",
    "        time_tag = t.find_all('span')\n",
    "    for t in time_tag:\n",
    "        time_dt.append(t.text)\n",
    "    for i in time_dt:\n",
    "        time_nw.append(i.replace(i[10:], ''))\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "media = []\n",
    "for i in range(len(headers)):\n",
    "    media.append('UDN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cl = []\n",
    "for i in time_dt:\n",
    "    time_cl.append(i.replace(i[10:], ''))\n",
    "    \n",
    "df_tyw_udn = pd.DataFrame(\n",
    "{\n",
    "    'titles' : headers,\n",
    "    'links' : news_url,\n",
    "    'time' : time_cl,\n",
    "    'media': media\n",
    "})\n",
    "df_tyw_udn.drop_duplicates('titles', 'first', inplace = True)\n",
    "df_tyw_udn.to_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/蔡英文_2020_udn.csv', \\\n",
    "                  encoding = 'utf_8_sig', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'media' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-898cad295764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mmedia_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UDN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'media' is not defined"
     ]
    }
   ],
   "source": [
    "# 韓國瑜 udn 2020\n",
    "url_h_list = ['https://udndata.com/ndapp/Searchdec?udndbid=udnfree&page={}&sharepage=50&select=1&kind=2&SearchString=%C1%FA%B0%EA%B7%EC%2B%A4%E9%B4%C1%3E%3D20191123%2B%A4%E9%B4%C1%3C%3D20200110%2B%B3%F8%A7O%3D%C1p%A6X%B3%F8%7C%B8g%C0%D9%A4%E9%B3%F8%7C%C1p%A6X%B1%DF%B3%F8%7CUpaper&_ga=2.91907998.544124342.1594709177-1030984037.1590050258'.format(i) for i in range(1, 22)]\n",
    "\n",
    "headers_h = []\n",
    "news_url_h = []\n",
    "time_dt_h = []\n",
    "time_nw_h = []\n",
    "\n",
    "for i in url_h_list:\n",
    "    udn_res = requests.get(i)\n",
    "    udn_soup = BeautifulSoup(udn_res.text, 'lxml')\n",
    "    udn_newslist= udn_soup.find_all('div', class_ = 'list')\n",
    "    for t in udn_newslist:\n",
    "        title_sect = t.find_all('h2')\n",
    "\n",
    "    for t in title_sect:\n",
    "        headers_h.append(t.find('a').text)\n",
    "\n",
    "    url_sect = [t.find('a') for t in title_sect]\n",
    "    for url in url_sect:\n",
    "        news_url_h.append('https://udn.com' + url['href'])\n",
    "\n",
    "    for t in udn_newslist:\n",
    "        time_tag = t.find_all('span')\n",
    "    for t in time_tag:\n",
    "        time_dt_h.append(t.text)\n",
    "    for i in time_dt_h:\n",
    "        time_nw_h.append(i.replace(i[10:], ''))\n",
    "\n",
    "    time.sleep(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1041"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_h = []\n",
    "for i in range(len(headers_h)):\n",
    "    media_h.append('UDN')\n",
    "len(media_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cl_h = []\n",
    "for i in time_dt_h:\n",
    "    time_cl_h.append(i.replace(i[10:], ''))\n",
    "    \n",
    "df_hky_udn = pd.DataFrame(\n",
    "{\n",
    "    'titles' : headers_h,\n",
    "    'links' : news_url_h,\n",
    "    'time' : time_cl_h,\n",
    "    'media': media_h\n",
    "})\n",
    "df_hky_udn.drop_duplicates('titles', 'first', inplace = True)\n",
    "df_hky_udn.to_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/韓國瑜_2020_udn.csv', \\\n",
    "                  encoding = 'utf_8_sig', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data organisation\n",
    "tsai_apple = pd.read_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/蔡英文_2020_applenews.csv')\n",
    "tsai_cht = pd.read_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/蔡英文_2020_chinatimes.csv')\n",
    "tsai_ldn = pd.read_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/蔡英文_2020_liberal.csv')\n",
    "tsai_udn = pd.read_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/蔡英文_2020_udn.csv')\n",
    "\n",
    "han_apple = pd.read_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/韓國瑜_2020_applenews.csv')\n",
    "han_cht = pd.read_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/韓國瑜_2020_chinatimes.csv')\n",
    "han_ldn = pd.read_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/韓國瑜_2020_liberal.csv')\n",
    "han_udn = pd.read_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/韓國瑜_2020_udn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_2020 = pd.concat([tsai_apple, tsai_cht, tsai_ldn, tsai_udn, han_apple, han_cht, han_ldn, han_udn])\n",
    "\n",
    "news_2020.drop_duplicates('titles', 'first', inplace = True)\n",
    "news_2020.to_csv('/Users/garday/Documents/MY499/Data Collection/Data/2020/news_2020.csv', \\\n",
    "                encoding = 'utf_8_sig', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
